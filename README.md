# Distributed ML Training using Data Parallelism

## Overview
This project implements distributed neural network training using PyTorch Distributed Data Parallel (DDP).

## Features
- Single Node Training Baseline
- Distributed Training using Data Parallelism
- MNIST Dataset
- Accuracy and Loss Tracking
- Training Time Measurement

---

## Installation

```bash
pip install -r requirements.txt

